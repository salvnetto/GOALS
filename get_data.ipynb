{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "#from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HISTORICO DE PARTIDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historico_partidas(tabela_url, paginas):\n",
    "  todas_partidas = []\n",
    "  data = requests.get(tabela_url) # Pega o html (em forma de string) da pagina 'Tabela do Brasileirao'\n",
    "  soup = BeautifulSoup(data.text) # Converte o String em html\n",
    "  tabela_brasileirao = soup.select('table.stats_table')[0] # Seleciona dentro do HTML a tabela em si\n",
    "  links_times = tabela_brasileirao.find_all('a') # Procura a Anchor que contem todos os links do time\n",
    "  links_times = [link.get(\"href\") for link in links_times] # Pega só os links em si\n",
    "  links_times = [link for link in links_times if paginas[0] in link] # Escolhe o link que leva para a pagina de Stats do time\n",
    "  times_urls = [f\"https://fbref.com{link}\" for link in links_times] # Transforma a string em URL\n",
    "\n",
    "  # Para cada time faça:\n",
    "  ## 1- Pega o nome do time\n",
    "  ## 2- Pega as informações das abas (em ordem):\n",
    "  ### Scores & Fixtures, Shooting, Goalkeeping, Passing, Pass Types, Goal and Shot Creation, Defensive Actions, Possession, Miscellaneous Stats\n",
    "  ## 3- Transforma a tabela que esta em HTML em Dataframe\n",
    "  for time_url in times_urls:\n",
    "    nome_time = time_url.split('/')[-1].replace(\"-Stats\", \"\")\n",
    "    data = requests.get(time_url) # Pega o html (em forma de string) da pagina do time\n",
    "    soup = BeautifulSoup(data.text)\n",
    "\n",
    "    ## Scores & Fixtures\n",
    "    partidas = pd.read_html(data.text, match= 'Scores & Fixtures')[0] # Transforma em dataframe o historico de partidas\n",
    "    \n",
    "    ## Shooting\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[1] in l]\n",
    "    data_shooting = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    shooting = pd.read_html(data_shooting.text, match= \"Shooting\")[0]\n",
    "    shooting.columns = shooting.columns.droplevel()\n",
    "    \n",
    "    ## Goalkeeping\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[2] in l]\n",
    "    data_goalkeeping = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    goalkeeping = pd.read_html(data_goalkeeping.text, match= \"Goalkeeping\")[0]\n",
    "    goalkeeping.columns = goalkeeping.columns.droplevel()\n",
    "    \n",
    "    ## Passing\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[3] in l]\n",
    "    data_passing = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    passing = pd.read_html(data_passing.text, match= \"Passing\")[0]\n",
    "    passing.columns = passing.columns.droplevel()\n",
    "    \n",
    "    ## Pass Types\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[4] in l]\n",
    "    data_passtype = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    pass_types = pd.read_html(data_passtype.text, match= \"Pass Types\")[0]\n",
    "    pass_types.columns = pass_types.columns.droplevel()\n",
    "    \n",
    "    ## Goal and Shot Creation\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[5] in l]\n",
    "    data_shotcreation = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    goal_shotcreation = pd.read_html(data_shotcreation.text, match= \"Goal and Shot Creation\")[0]\n",
    "    goal_shotcreation.columns = goal_shotcreation.columns.droplevel()\n",
    "    \n",
    "    ## Defensive Actions\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[6] in l]\n",
    "    data_defensive = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    defensive = pd.read_html(data_defensive.text, match= \"Defensive Actions\")[0]\n",
    "    defensive.columns = defensive.columns.droplevel()\n",
    "    \n",
    "    ## Possession\n",
    "    ## Miscellaneous Stats\n",
    "\n",
    "    # Merge das abas\n",
    "    try:\n",
    "      time_df = partidas.merge(\n",
    "        shooting[['Date', 'Sh', 'SoT', 'SoT%', 'G/Sh', 'G/SoT', 'FK', 'PK', 'PKatt', 'npxG', 'npxG/Sh', 'G-xG', 'np:G-xG']], on= 'Date')\n",
    "      time_df = time_df.merge(\n",
    "        goalkeeping[['Date', 'SoTA', 'Saves', 'Save%', 'CS', 'PSxG', 'PSxG+/-']], on= 'Date')\n",
    "      time_df = time_df.merge(\n",
    "        passing[['Date', 'TotDist', 'PrgDist', 'Ast',\t'xAG', 'xA', 'KP', '1/3', 'PPA', 'CrsPA', 'PrgP']], on= 'Date') \n",
    "      time_df = time_df.merge(\n",
    "        pass_types[['Date', 'TB', 'Sw', 'Crs', 'Blocks']], on= 'Date')\n",
    "      time_df = time_df.merge(\n",
    "        goal_shotcreation[['Date', 'SCA', 'GCA']], on= 'Date')\n",
    "      time_df = time_df.merge(\n",
    "        defensive[['Date', 'TklW','Def 3rd','Mid 3rd','Att 3rd','Int','Err']], on= 'Date')\n",
    "      #time_df = time_df.merge(\n",
    "        #possession[['Date', ]], on= 'Date')\n",
    "      #time_df = time_df.merge(\n",
    "        #miscellaneous[['Date',]], on= 'Date')\n",
    "    except ValueError:\n",
    "      continue\n",
    "    \n",
    "    time_df = time_df[time_df[\"Comp\"] == \"Série A\"] # Filtra o brasileirão\n",
    "    time_df['Equipe'] = nome_time\n",
    "    todas_partidas.append(time_df)\n",
    "    time.sleep(10)\n",
    "\n",
    "  historico = pd.concat(todas_partidas)\n",
    "  return historico\n",
    "\n",
    "tabela = \"https://fbref.com/en/comps/24/Serie-A-Stats\"\n",
    "abas = ['/squads/', 'all_comps/shooting/', 'all_comps/keeper', 'all_comps/passing', 'all_comps/passing_types', 'all_comps/gca', 'all_comps/defense', 'all_comps/possession', 'all_comps/misc']\n",
    "historico = historico_partidas(tabela, abas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_acentos(texto):\n",
    "    return unidecode(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico.drop(\n",
    "  ['Notes','Captain', 'Formation', 'Referee', 'Match Report'], \n",
    "  axis= 'columns', inplace= True)\n",
    "historico[['GF', 'GA']] = historico[['GF', 'GA']].astype(int)\n",
    "historico['Opponent'] = historico['Opponent'].apply(remover_acentos)\n",
    "\n",
    "substitutions = {\n",
    "    'Botafogo-RJ': 'Botafogo (RJ)',\n",
    "    'Gremio': 'Gremio',\n",
    "    'Athletico-Paranaense': 'Ath Paranaense',\n",
    "    'Atletico-Mineiro': 'Atletico Mineiro',\n",
    "    'Sao-Paulo': 'Sao Paulo',\n",
    "    'Vasco-da-Gama': 'Vasco da Gama',\n",
    "    'America-MG': 'America (MG)'\n",
    "}\n",
    "\n",
    "for key, value in substitutions.items():\n",
    "    historico['Equipe'] = historico['Equipe'].str.replace(unidecode(key), value, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico_casa = historico[historico['Venue'] == 'Home']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar planilha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ultimo_jogo = historico['Date'].max()\n",
    "historico.to_excel(f'dados/historico/{data_ultimo_jogo}.xlsx', sheet_name='historico', index=False)\n",
    "historico_casa.to_excel(f'dados/historico/casa-{data_ultimo_jogo}.xlsx', sheet_name='historico', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABELA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabela_brasileirao(tabela_url):\n",
    "  data = requests.get(tabela_url)\n",
    "  tabela = pd.read_html(data.text, match= 'Regular season')[0]\n",
    "  return tabela\n",
    "\n",
    "tabela_url = \"https://fbref.com/en/comps/24/Serie-A-Stats\"\n",
    "tabela_df = tabela_brasileirao(tabela_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_df.drop(['Attendance', 'Top Team Scorer', 'Goalkeeper', 'Notes'], axis= 'columns', inplace= True)\n",
    "tabela_df['xGConv'] = tabela_df['GF'] - tabela_df['xG']   #Conversão de xG\n",
    "tabela_df['xGAConv'] = tabela_df['GA'] - tabela_df['xGA']   #Conversão de xGA\n",
    "tabela_df['GF/MP'] = tabela_df['GF']/tabela_df['MP']    #Gols feitos por partida jogada\n",
    "tabela_df['GA/MP'] = tabela_df['GA']/tabela_df['MP']    #Gols recebidos por partida jogada\n",
    "tabela_df['Squad'] = tabela_df['Squad'].apply(remover_acentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar planilha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodada = tabela_df['MP'].max()\n",
    "tabela_df.to_excel(f'dados/tabela/rodada-{rodada}.xlsx', sheet_name='tabela', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historico por time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodada = tabela['MP'].max()\n",
    "if not os.path.exists(f'dados/historico/times/rodada_{rodada}'):\n",
    "  os.makedirs(f'dados/historico/times/rodada_{rodada}')\n",
    "\n",
    "for time in historico_casa['Equipe']:\n",
    "  df = historico_casa[historico_casa['Equipe'] == time]\n",
    "  df = pd.concat([df, historico_casa[historico_casa['Opponent'] == time]])\n",
    "  df.to_excel(f'dados/historico/times/rodada_{rodada}/{time}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximas Rodadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxs_rodadas(tabela_url):\n",
    "  data = requests.get(tabela_url)\n",
    "  tabela = pd.read_html(data.text, match= 'Scores & Fixtures')[0]\n",
    "  return tabela\n",
    "\n",
    "rodadas_url = \"https://fbref.com/en/comps/24/schedule/Serie-A-Scores-and-Fixtures\"\n",
    "rodadas_df = proxs_rodadas(rodadas_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodadas_df = rodadas_df.loc[rodadas_df['Home'] != 'nan']\n",
    "rodadas_df = rodadas_df.loc[rodadas_df['Away'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodadas_df['Home'] = rodadas_df['Home'].apply(remover_acentos)\n",
    "rodadas_df['Away'] = rodadas_df['Away'].apply(remover_acentos)\n",
    "\n",
    "substitutions = {\n",
    "    'Botafogo-RJ': 'Botafogo (RJ)',\n",
    "    'Gremio': 'Gremio',\n",
    "    'Athletico-Paranaense': 'Ath Paranaense',\n",
    "    'Atletico-Mineiro': 'Atletico Mineiro',\n",
    "    'Sao-Paulo': 'Sao Paulo',\n",
    "    'Vasco-da-Gama': 'Vasco da Gama',\n",
    "    'America-MG': 'America (MG)'\n",
    "}\n",
    "\n",
    "for key, value in substitutions.items():\n",
    "    rodadas_df['Home'] = rodadas_df['Home'].str.replace(unidecode(key), value, regex=True)\n",
    "    rodadas_df['Away'] = rodadas_df['Away'].str.replace(unidecode(key), value, regex=True)\n",
    "\n",
    "rodadas_df.drop(['Time', 'Attendance', 'Venue', 'Referee', 'Match Report', 'Notes'], axis= 1, inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodadas_df.to_excel(f'dados/previsoes/rodadas.xlsx', sheet_name='tabela', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
