{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leagues\n",
    "\n",
    "1 - Brasileirao A \\\n",
    "2 - Premier League \\\n",
    "3 - Serie A TIM \\\n",
    "4 - Bundesliga \\\n",
    "5 - La Liga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligas = {\n",
    "    'brasileirao_a': [\"Série A\", \"https://fbref.com/en/comps/24/Serie-A-Stats\", 'Brasileirao', 'https://fbref.com/en/comps/24/schedule/Serie-A-Scores-and-Fixtures'],\n",
    "    'premier_league': [\"Premier League\", \"https://fbref.com/en/comps/9/Premier-League-Stats\", 'Premier_League', 'https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures'],\n",
    "    'serie_a_tim': [\"Serie A\", \"https://fbref.com/en/comps/11/Serie-A-Stats\", 'Serie_A_TIM', \"https://fbref.com/en/comps/11/schedule/Serie-A-Scores-and-Fixtures\"],\n",
    "    'bundesliga': [\"Bundesliga\", \"https://fbref.com/en/comps/20/Bundesliga-Stats\", 'Bundesliga', \"https://fbref.com/en/comps/20/schedule/Bundesliga-Scores-and-Fixtures\"],\n",
    "    'la_liga': [\"La Liga\", \"https://fbref.com/en/comps/12/La-Liga-Stats\", 'La_Liga', \"https://fbref.com/en/comps/12/schedule/La-Liga-Scores-and-Fixtures\"]\n",
    "}\n",
    "\n",
    "\n",
    "anos = list(range(2023, 2021, -1))\n",
    "\n",
    "season_atual = 2023\n",
    "liga_atual = ligas['premier_league']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_history(league, years = [2023], update = None):\n",
    "  print(\"Getting the seasons: \", years)\n",
    "  liga = league[1]\n",
    "  match_history = []\n",
    "  for year in years:\n",
    "    print(\"Scraping the season: \", year)\n",
    "    data = requests.get(liga)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    tabela = soup.select('table.stats_table')[0]            #Seleciona a tabela principal\n",
    "    links = tabela.find_all('a')                            #Procura a Anchor que contem todos os links do time\n",
    "    links = [link.get('href') for link in links]            #Pega os links (sem o começo deles)\n",
    "    links = [link for link in links if '/squads/' in link]  #Pega apenas o link 'squads'\n",
    "    urls = [f\"https://fbref.com{link}\" for link in links]   #Adiciona o inicio do html\n",
    "\n",
    "    try:\n",
    "      prev_season = soup.select('a.prev')[0].get('href')      #Vai para a temporada anterior\n",
    "      liga = f\"https://fbref.com{prev_season}\"\n",
    "    except IndexError:\n",
    "      continue\n",
    "\n",
    "    #Itera sobre todos os times da tabela\n",
    "    for team in urls:\n",
    "      print(team)\n",
    "      nome_time = team.split('/')[-1].replace('-Stats', '').replace('-','_').lower()\n",
    "      data = requests.get(team)\n",
    "      soup = BeautifulSoup(data.text)\n",
    "\n",
    "      ##Partidas jogadas\n",
    "      matches = pd.read_html(data.text, match= 'Scores & Fixtures')[0]\n",
    "\n",
    "      ##Chutes\n",
    "      links = [link.get(\"href\") for link in soup.find_all('a')]\n",
    "      links = [link for link in links if link and 'all_comps/shooting/' in link]\n",
    "      data_shooting = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      shooting = pd.read_html(data_shooting.text, match= \"Shooting\")[0]\n",
    "      shooting.columns = shooting.columns.droplevel()\n",
    "      time.sleep(2)\n",
    "\n",
    "      ##Goleiros\n",
    "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      links = [l for l in links if l and 'all_comps/keeper' in l]\n",
    "      data_goalkeeping = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      goalkeeping = pd.read_html(data_goalkeeping.text, match= \"Goalkeeping\")[0]\n",
    "      goalkeeping.columns = goalkeeping.columns.droplevel()\n",
    "      time.sleep(2)\n",
    "\n",
    "      ##Passes\n",
    "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      links = [l for l in links if l and 'all_comps/passing' in l]\n",
    "      data_passing = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      passing = pd.read_html(data_passing.text, match= \"Passing\")[0]\n",
    "      passing.columns = passing.columns.droplevel()\n",
    "      time.sleep(2)\n",
    "\n",
    "      ##Tipos de Passes\n",
    "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      links = [l for l in links if l and 'all_comps/passing_types' in l]\n",
    "      data_passtype = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      pass_types = pd.read_html(data_passtype.text, match= \"Pass Types\")[0]\n",
    "      pass_types.columns = pass_types.columns.droplevel()\n",
    "      time.sleep(2)\n",
    "\n",
    "      ##Gols e Criação de Chutes\n",
    "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      links = [l for l in links if l and 'all_comps/gca' in l]\n",
    "      data_shotcreation = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      goal_shotcreation = pd.read_html(data_shotcreation.text, match= \"Goal and Shot Creation\")[0]\n",
    "      goal_shotcreation.columns = goal_shotcreation.columns.droplevel()\n",
    "      time.sleep(2)\n",
    "\n",
    "      ##Acoes Defensivas\n",
    "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      links = [l for l in links if l and 'all_comps/defense' in l]\n",
    "      data_defensive = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      defensive = pd.read_html(data_defensive.text, match= \"Defensive Actions\")[0]\n",
    "      defensive.columns = defensive.columns.droplevel()\n",
    "      time.sleep(2)\n",
    "\n",
    "      ##Posse de Bola\n",
    "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      links = [l for l in links if l and 'all_comps/possession' in l]\n",
    "      data_possession = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      possession = pd.read_html(data_possession.text, match= \"Possession\")[0]\n",
    "      possession.columns = possession.columns.droplevel()\n",
    "      time.sleep(2)\n",
    "\n",
    "      ##Demais Estatisticas\n",
    "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      links = [l for l in links if l and 'all_comps/misc' in l]\n",
    "      data_misc = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      misc = pd.read_html(data_misc.text, match= \"Miscellaneous Stats\")[0]\n",
    "      misc.columns = misc.columns.droplevel()\n",
    "      time.sleep(2)\n",
    "\n",
    "      # Merge das abas\n",
    "      try:\n",
    "        matches_df = matches.merge(\n",
    "          shooting[['Date', 'Sh', 'SoT']], on= 'Date')\n",
    "        matches_df = matches_df.merge(\n",
    "          goalkeeping[['Date', 'Saves']], on= 'Date')\n",
    "        matches_df = matches_df.merge(\n",
    "          passing[['Date', 'Cmp', 'Att', 'PrgP', 'KP', '1/3']], on= 'Date')\n",
    "        matches_df.rename(columns={'1/3': 'pass_3rd'}, inplace=True) \n",
    "        matches_df = matches_df.merge(\n",
    "          pass_types[['Date', 'Sw', 'Crs']], on= 'Date')\n",
    "        matches_df = matches_df.merge(\n",
    "          goal_shotcreation[['Date', 'SCA', 'GCA']], on= 'Date')\n",
    "        matches_df = matches_df.merge(\n",
    "          defensive[['Date', 'Tkl', 'TklW', 'Def 3rd', 'Att 3rd', 'Blocks', 'Int']], on= 'Date')\n",
    "        matches_df.rename(columns={'Att 3rd': 'Tkl_Att_3rd',\n",
    "                                   'Def 3rd': 'Tkl_Def_3rd'}, inplace=True)\n",
    "        matches_df = matches_df.merge(\n",
    "          possession[['Date', 'Att 3rd']], on= 'Date')\n",
    "        matches_df.rename(columns={'Att 3rd': 'Touches_Att_3rd'}, inplace=True)\n",
    "        matches_df = matches_df.merge(\n",
    "          misc[['Date', 'Fls', 'Off', 'Recov']], on= 'Date')\n",
    "      except ValueError:\n",
    "        continue\n",
    "\n",
    "      matches_df = matches_df[matches_df[\"Comp\"] == league[0]]\n",
    "      matches_df['Season'] = year\n",
    "      matches_df['Team'] = nome_time\n",
    "      \n",
    "      match_history.append(matches_df)\n",
    "      time.sleep(4)\n",
    "    \n",
    "  match_history = pd.concat(match_history)\n",
    "  match_history.columns = [c.lower() for c in match_history.columns]\n",
    "  if update is not None:\n",
    "    update.drop(update[update['season'] == year].index, inplace= True)\n",
    "    update.columns = [c.lower() for c in update.columns]\n",
    "    match_history = pd.concat([match_history, update], axis = 0)\n",
    "\n",
    "  return match_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standings(league, years= [2023], update = None):\n",
    "  table = []\n",
    "  liga = league[1]\n",
    "  print(\"Getting the seasons: \", years)\n",
    "  for year in years:\n",
    "    print(\"Scraping the season: \", year)\n",
    "    data = requests.get(liga)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    table_df = pd.read_html(data.text, match= 'Regular season')[0]\n",
    "    table_df['Season'] = year\n",
    "    table_df['league_name'] = league[2]\n",
    "  \n",
    "    table.append(table_df)\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "      prev_season = soup.select('a.prev')[0].get('href')      #Vai para a temporada anterior\n",
    "      liga = f\"https://fbref.com{prev_season}\"\n",
    "    except IndexError:\n",
    "      continue\n",
    "  \n",
    "  table = pd.concat(table)\n",
    "  table.columns = [c.lower() for c in table.columns]\n",
    "  if update is not None:\n",
    "    update.drop(update[update['season'] == year].index, inplace= True)\n",
    "    update.columns = [c.lower() for c in update.columns]\n",
    "    table = pd.concat([table, update], axis = 0)\n",
    "\n",
    "  return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_matches(league):\n",
    "  data = requests.get(league[3])\n",
    "  matches = pd.read_html(data.text, match= 'Scores & Fixtures')[0]\n",
    "  matches['league_name'] = league[2]\n",
    "  matches.columns = [c.lower() for c in matches.columns]\n",
    "  return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squads(league, years = [2023], update = None):\n",
    "  print(\"Getting the seasons: \", years)\n",
    "  liga = league[1]\n",
    "  squads = []\n",
    "  for year in years:\n",
    "    print(\"Scraping the season: \", year)\n",
    "    data = requests.get(liga)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    tabela = soup.select('table.stats_table')[0]            #Seleciona a tabela principal\n",
    "    links = tabela.find_all('a')                            #Procura a Anchor que contem todos os links do time\n",
    "    links = [link.get('href') for link in links]            #Pega os links (sem o começo deles)\n",
    "    links = [link for link in links if '/squads/' in link]  #Pega apenas o link 'squads'\n",
    "    urls = [f\"https://fbref.com{link}\" for link in links]   #Adiciona o inicio do html\n",
    "\n",
    "    try:\n",
    "      prev_season = soup.select('a.prev')[0].get('href')      #Vai para a temporada anterior\n",
    "      liga = f\"https://fbref.com{prev_season}\"\n",
    "    except IndexError:\n",
    "      continue\n",
    "\n",
    "    #Itera sobre todos os times da tabela\n",
    "    for team in urls:\n",
    "      print(team)\n",
    "      nome_time = team.split('/')[-1].replace('-Stats', '').replace('-','_').lower()\n",
    "      data = requests.get(team)\n",
    "      soup = BeautifulSoup(data.text)\n",
    "\n",
    "      ##Elenco\n",
    "      squads_df = pd.read_html(data.text, match= 'Standard Stats')[0]\n",
    "      squads_df.columns = squads_df.columns.droplevel()\n",
    "\n",
    "      squads_df['Season'] = year\n",
    "      squads_df['Team'] = nome_time\n",
    "      squads_df['League'] = league[2]\n",
    "      squads_df = squads_df.iloc[:-2]\n",
    "      try:\n",
    "        coach = str(soup.find_all('p')[6])\n",
    "        squads_df['coach'] = coach.split('<')[3].split('> ')[1]\n",
    "      except IndexError:\n",
    "        squads_df['coach'] = None\n",
    "      print(squads_df)\n",
    "      squads.append(squads_df)\n",
    "\n",
    "  squads = pd.concat(squads)\n",
    "  squads.columns = [c.lower() for c in squads.columns]\n",
    "  if update is not None:\n",
    "    update.drop(update[update['season'] == year].index, inplace= True)\n",
    "    update.columns = [c.lower() for c in update.columns]\n",
    "    squads = pd.concat([squads, update], axis = 0)\n",
    "\n",
    "  return squads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixar anos anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  historico.append(match_history(ligas[liga], years= anos))\n",
    "\n",
    "historico = pd.concat(historico)\n",
    "\n",
    "historico.to_excel(f'../datasets/xlsx/historico(naotratado).xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  tabela.append(standings(ligas[liga], years= anos))\n",
    "\n",
    "tabela = pd.concat(tabela)\n",
    "\n",
    "tabela.to_excel(f'../datasets/xlsx/tabela(naotratado).xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodadas = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  rodadas.append(next_matches(ligas[liga]))\n",
    "\n",
    "rodadas = pd.concat(rodadas)\n",
    "\n",
    "rodadas.to_excel(f'../datasets/xlsx/rodadas(naotratado).xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping the league: Brasileirao\n",
      "Getting the seasons:  [2023]\n",
      "Scraping the season:  2023\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mg:\\Meu Drive\\Bet\\scraper\\scraping.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m liga \u001b[39min\u001b[39;00m ligas:\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mScraping the league: \u001b[39m\u001b[39m{\u001b[39;00mligas[liga][\u001b[39m2\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   elenco\u001b[39m.\u001b[39mappend(get_squads(ligas[liga]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m elenco \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(elenco)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m elenco\u001b[39m.\u001b[39mto_excel(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../datasets/xlsx/elenco(naotratado).xlsx\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mg:\\Meu Drive\\Bet\\scraper\\scraping.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m data \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(liga)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(data\u001b[39m.\u001b[39mtext)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tabela \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mselect(\u001b[39m'\u001b[39;49m\u001b[39mtable.stats_table\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m]            \u001b[39m#Seleciona a tabela principal\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m links \u001b[39m=\u001b[39m tabela\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)                            \u001b[39m#Procura a Anchor que contem todos os links do time\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m links \u001b[39m=\u001b[39m [link\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m links]            \u001b[39m#Pega os links (sem o começo deles)\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "elenco = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  elenco.append(get_squads(ligas[liga]))\n",
    "\n",
    "elenco = pd.concat(elenco)\n",
    "\n",
    "elenco.to_excel(f'../datasets/xlsx/elenco(naotratado).xlsx', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atualizar temporada atual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico_old = pd.read_excel(f'../datasets/xlsx/historico(naotratado).xlsx')\n",
    "\n",
    "historico = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  historico.append(match_history(liga_atual, update= historico_old))\n",
    "\n",
    "historico = pd.concat(historico)\n",
    "\n",
    "historico.to_excel(f'datasets/xlsx/historico(naotratado).xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_old = pd.read_excel(f'../datasets/xlsx/tabela(naotratado).xlsx')\n",
    "\n",
    "tabela = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  tabela.append(match_history(liga_atual, update= tabela_old))\n",
    "\n",
    "tabela = pd.concat(tabela)\n",
    "\n",
    "tabela.to_excel(f'datasets/xlsx/tabela(naotratado).xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elenco_old = pd.read_excel(f'../datasets/xlsx/elenco(naotratado).xlsx')\n",
    "\n",
    "elenco = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  elenco.append(get_squads(liga_atual, update= elenco_old))\n",
    "\n",
    "elenco = pd.concat(elenco)\n",
    "\n",
    "elenco.to_excel(f'datasets/xlsx/elenco(naotratado).xlsx', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
