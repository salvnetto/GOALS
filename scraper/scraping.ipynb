{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leagues\n",
    "\n",
    "1 - Brasileirao A \\\n",
    "2 - Premier League \\\n",
    "3 - Serie A TIM \\\n",
    "4 - Bundesliga \\\n",
    "5 - La Liga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligas = {\n",
    "    'brasileirao_a': [\"Série A\", \"https://fbref.com/en/comps/24/Serie-A-Stats\", 'Brasileirao', 'https://fbref.com/en/comps/24/schedule/Serie-A-Scores-and-Fixtures'],\n",
    "    'premier_league': [\"Premier League\", \"https://fbref.com/en/comps/9/Premier-League-Stats\", 'Premier_League', 'https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures'],\n",
    "    'serie_a_tim': [\"Serie A\", \"https://fbref.com/en/comps/11/Serie-A-Stats\", 'Serie_A_TIM', \"https://fbref.com/en/comps/11/schedule/Serie-A-Scores-and-Fixtures\"],\n",
    "    'bundesliga': [\"Bundesliga\", \"https://fbref.com/en/comps/20/Bundesliga-Stats\", 'Bundesliga', \"https://fbref.com/en/comps/20/schedule/Bundesliga-Scores-and-Fixtures\"],\n",
    "    'la_liga': [\"La Liga\", \"https://fbref.com/en/comps/12/La-Liga-Stats\", 'La_Liga', \"https://fbref.com/en/comps/12/schedule/La-Liga-Scores-and-Fixtures\"],\n",
    "    'ligue_1': [\"Ligue 1\", \"https://fbref.com/en/comps/13/Ligue-1-Stats\", 'Ligue_1', \"https://fbref.com/en/comps/13/schedule/Ligue-1-Scores-and-Fixtures\"]\n",
    "}\n",
    "\n",
    "\n",
    "anos = list(range(2023, 2014, -1))\n",
    "#anos.pop(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_history(league, years = [2023], update = None):\n",
    "  print(\"Getting the seasons: \", years)\n",
    "  liga = league[1]\n",
    "  match_history = []\n",
    "  per = 0\n",
    "  for year in years:\n",
    "    print(\"Scraping the season: \", year)\n",
    "    data = requests.get(liga)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    tabela = soup.select('table.stats_table')[0]            #Seleciona a tabela principal\n",
    "    links = tabela.find_all('a')                            #Procura a Anchor que contem todos os links do time\n",
    "    links = [link.get('href') for link in links]            #Pega os links (sem o começo deles)\n",
    "    links = [link for link in links if '/squads/' in link]  #Pega apenas o link 'squads'\n",
    "    urls = [f\"https://fbref.com{link}\" for link in links]   #Adiciona o inicio do html\n",
    "\n",
    "    try:\n",
    "      prev_season = soup.select('a.prev')[0].get('href')      #Vai para a temporada anterior\n",
    "      liga = f\"https://fbref.com{prev_season}\"\n",
    "    except IndexError:\n",
    "      print(f'***Erro para o ano: {year}')\n",
    "      continue\n",
    "\n",
    "    perx = 0\n",
    "    #Itera sobre todos os times da tabela\n",
    "    for team in urls:\n",
    "      nome_time = team.split('/')[-1].replace('-Stats', '').replace('-','_').lower()\n",
    "      per+=1\n",
    "      perx+=1\n",
    "      percent_total = round((per/(len(urls)*len(years)))*100, 2)\n",
    "      print(f'-> {percent_total}% ({perx}/{len(urls)} {nome_time})')\n",
    "      data = requests.get(team)\n",
    "      soup = BeautifulSoup(data.text)\n",
    "\n",
    "      ##Partidas jogadas\n",
    "      matches = pd.read_html(data.text, match= 'Scores & Fixtures')[0]\n",
    "\n",
    "      try:\n",
    "        ##Chutes\n",
    "        links = [link.get(\"href\") for link in soup.find_all('a')]\n",
    "        links = [link for link in links if link and 'all_comps/shooting/' in link]\n",
    "        data_shooting = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        shooting = pd.read_html(data_shooting.text, match= \"Shooting\")[0]\n",
    "        shooting.columns = shooting.columns.droplevel()\n",
    "        matches_df = matches.merge(\n",
    "          shooting[['Date', 'Sh', 'SoT']], on= 'Date')\n",
    "        time.sleep(2)\n",
    "      except (ValueError, IndexError):\n",
    "        pass\n",
    "\n",
    "      try:\n",
    "        ##Goleiros\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/keeper' in l]\n",
    "        data_goalkeeping = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        goalkeeping = pd.read_html(data_goalkeeping.text, match= \"Goalkeeping\")[0]\n",
    "        goalkeeping.columns = goalkeeping.columns.droplevel()\n",
    "        matches_df = matches_df.merge(\n",
    "          goalkeeping[['Date', 'Saves']], on= 'Date')\n",
    "        time.sleep(2)\n",
    "      except (ValueError, IndexError):\n",
    "        pass\n",
    "      \n",
    "      try:\n",
    "        ##Passes\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/passing' in l]\n",
    "        data_passing = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        passing = pd.read_html(data_passing.text, match= \"Passing\")[0]\n",
    "        passing.columns = passing.columns.droplevel()\n",
    "        matches_df = matches_df.merge(\n",
    "          passing[['Date', 'Cmp', 'Att', 'PrgP', 'KP', '1/3']], on= 'Date')\n",
    "        matches_df.rename(columns={'1/3': 'pass_3rd'}, inplace=True)\n",
    "        time.sleep(2)\n",
    "      except (ValueError, IndexError):\n",
    "        pass\n",
    "      \n",
    "      try:\n",
    "        ##Tipos de Passes\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/passing_types' in l]\n",
    "        data_passtype = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        pass_types = pd.read_html(data_passtype.text, match= \"Pass Types\")[0]\n",
    "        pass_types.columns = pass_types.columns.droplevel()\n",
    "        matches_df = matches_df.merge(\n",
    "          pass_types[['Date', 'Sw', 'Crs']], on= 'Date')\n",
    "        time.sleep(2)\n",
    "      except (ValueError, IndexError):\n",
    "        pass\n",
    "\n",
    "      try:\n",
    "        ##Gols e Criação de Chutes\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/gca' in l]\n",
    "        data_shotcreation = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        goal_shotcreation = pd.read_html(data_shotcreation.text, match= \"Goal and Shot Creation\")[0]\n",
    "        goal_shotcreation.columns = goal_shotcreation.columns.droplevel()\n",
    "        matches_df = matches_df.merge(\n",
    "          goal_shotcreation[['Date', 'SCA', 'GCA']], on= 'Date')\n",
    "        time.sleep(2)\n",
    "      except (ValueError, IndexError):\n",
    "        pass\n",
    "\n",
    "      try:\n",
    "        ##Acoes Defensivas\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/defense' in l]\n",
    "        data_defensive = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        defensive = pd.read_html(data_defensive.text, match= \"Defensive Actions\")[0]\n",
    "        defensive.columns = defensive.columns.droplevel()\n",
    "        matches_df = matches_df.merge(\n",
    "          defensive[['Date', 'Tkl', 'TklW', 'Def 3rd', 'Att 3rd', 'Blocks', 'Int']], on= 'Date')\n",
    "        matches_df.rename(columns={'Att 3rd': 'Tkl_Att_3rd',\n",
    "                                   'Def 3rd': 'Tkl_Def_3rd'}, inplace=True)\n",
    "        time.sleep(2)\n",
    "      except (ValueError, IndexError):\n",
    "        pass\n",
    "\n",
    "      try:\n",
    "        ##Posse de Bola\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/possession' in l]\n",
    "        data_possession = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        possession = pd.read_html(data_possession.text, match= \"Possession\")[0]\n",
    "        possession.columns = possession.columns.droplevel()\n",
    "        matches_df = matches_df.merge(\n",
    "          possession[['Date', 'Att 3rd']], on= 'Date')\n",
    "        matches_df.rename(columns={'Att 3rd': 'Touches_Att_3rd'}, inplace=True)\n",
    "        time.sleep(2)\n",
    "      except (ValueError, IndexError):\n",
    "        pass\n",
    "\n",
    "      try:\n",
    "        ##Demais Estatisticas\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/misc' in l]\n",
    "        data_misc = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        misc = pd.read_html(data_misc.text, match= \"Miscellaneous Stats\")[0]\n",
    "        misc.columns = misc.columns.droplevel()\n",
    "        if 'Recov' not in misc.columns:\n",
    "          misc['Recov'] = np.nan\n",
    "        matches_df = matches_df.merge(\n",
    "          misc[['Date', 'Fls', 'Off', 'Recov']], on= 'Date')\n",
    "        time.sleep(2)\n",
    "      except (ValueError, IndexError):\n",
    "        pass\n",
    "\n",
    "\n",
    "      matches_df = matches_df[matches_df[\"Comp\"] == league[0]]\n",
    "      matches_df['Season'] = year\n",
    "      matches_df['Team'] = nome_time\n",
    "      \n",
    "      match_history.append(matches_df)\n",
    "      time.sleep(4)\n",
    "    \n",
    "  print(match_history[0].columns)\n",
    "  print(match_history[19].columns)\n",
    "  match_history = pd.concat(match_history)\n",
    "  match_history.columns = [c.lower() for c in match_history.columns]\n",
    "  if update is not None:\n",
    "    update.drop(update[update['season'] == year].index, inplace= True)\n",
    "    update.columns = [c.lower() for c in update.columns]\n",
    "    match_history = pd.concat([match_history, update], axis = 0)\n",
    "\n",
    "  return match_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standings(league, years= [2023], update = None):\n",
    "  table = []\n",
    "  liga = league[1]\n",
    "  print(\"Getting the seasons: \", years)\n",
    "  for year in years:\n",
    "    print(\"Scraping the season: \", year)\n",
    "    data = requests.get(liga)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    table_df = pd.read_html(data.text, match= 'Regular season')[0]\n",
    "    table_df['Season'] = year\n",
    "    table_df['league_name'] = league[2]\n",
    "  \n",
    "    table.append(table_df)\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "      prev_season = soup.select('a.prev')[0].get('href')      #Vai para a temporada anterior\n",
    "      liga = f\"https://fbref.com{prev_season}\"\n",
    "    except IndexError:\n",
    "      print('*Erro no ano: ', year)\n",
    "      continue\n",
    "  \n",
    "  table = pd.concat(table)\n",
    "  table.columns = [c.lower() for c in table.columns]\n",
    "  if update is not None:\n",
    "    update.drop(update[update['season'] == year].index, inplace= True)\n",
    "    update.columns = [c.lower() for c in update.columns]\n",
    "    table = pd.concat([table, update], axis = 0)\n",
    "\n",
    "  return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_matches(league):\n",
    "  data = requests.get(league[3])\n",
    "  matches = pd.read_html(data.text, match= 'Scores & Fixtures')[0]\n",
    "  matches['league_name'] = league[2]\n",
    "  matches.columns = [c.lower() for c in matches.columns]\n",
    "  return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squads(league, years = [2023], update = None):\n",
    "  print(\"Getting the seasons: \", years)\n",
    "  liga = league[1]\n",
    "  squads = []\n",
    "  per = 0\n",
    "  for year in years:\n",
    "    print(\"Scraping the season: \", year)\n",
    "    data = requests.get(liga)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    tabela = soup.select('table.stats_table')[0]            #Seleciona a tabela principal\n",
    "    links = tabela.find_all('a')                            #Procura a Anchor que contem todos os links do time\n",
    "    links = [link.get('href') for link in links]            #Pega os links (sem o começo deles)\n",
    "    links = [link for link in links if '/squads/' in link]  #Pega apenas o link 'squads'\n",
    "    urls = [f\"https://fbref.com{link}\" for link in links]   #Adiciona o inicio do html\n",
    "\n",
    "    try:\n",
    "      prev_season = soup.select('a.prev')[0].get('href')      #Vai para a temporada anterior\n",
    "      liga = f\"https://fbref.com{prev_season}\"\n",
    "    except IndexError:\n",
    "      print(f'* Erro para o ano: {year}')\n",
    "      continue\n",
    "\n",
    "    #Itera sobre todos os times da tabela\n",
    "    perx = 0\n",
    "    for team in urls:\n",
    "      nome_time = team.split('/')[-1].replace('-Stats', '').replace('-','_').lower()\n",
    "      per+=1\n",
    "      perx+=1\n",
    "      percent_total = round((per/(len(urls)*len(years)))*100, 2)\n",
    "      print(f'-> {percent_total}% ({perx}/{len(urls)} {nome_time})')\n",
    "      data = requests.get(team)\n",
    "      soup = BeautifulSoup(data.text)\n",
    "\n",
    "      ##Elenco\n",
    "      squads_df = pd.read_html(data.text, match= 'Standard Stats')[0]\n",
    "      squads_df.columns = squads_df.columns.droplevel()\n",
    "\n",
    "      squads_df['Season'] = year\n",
    "      squads_df['Team'] = nome_time\n",
    "      squads_df['League'] = league[2]\n",
    "      squads_df = squads_df.iloc[:-2]\n",
    "      try:\n",
    "        coach = str(soup.find_all('p')[6])\n",
    "        squads_df['coach'] = coach.split('<')[3].split('> ')[1]\n",
    "      except IndexError:\n",
    "        squads_df['coach'] = None\n",
    "      squads_df = squads_df.loc[:,~squads_df.columns.duplicated(keep= 'first')]\n",
    "      squads.append(squads_df)\n",
    "      time.sleep(5)\n",
    "\n",
    "  squads = pd.concat(squads)\n",
    "  squads.columns = [c.lower() for c in squads.columns]\n",
    "  if update is not None:\n",
    "    update.drop(update[update['season'] == year].index, inplace= True)\n",
    "    update.columns = [c.lower() for c in update.columns]\n",
    "    squads = pd.concat([squads, update], axis = 0)\n",
    "\n",
    "  return squads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixar anos anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 5.0% (2/20 real_madrid)\n",
      "-> 7.5% (3/20 barcelona)\n",
      "-> 10.0% (4/20 atletico_madrid)\n",
      "-> 12.5% (5/20 athletic_club)\n",
      "-> 15.0% (6/20 real_sociedad)\n",
      "-> 17.5% (7/20 real_betis)\n",
      "-> 20.0% (8/20 las_palmas)\n",
      "-> 22.5% (9/20 valencia)\n",
      "-> 25.0% (10/20 rayo_vallecano)\n",
      "-> 27.5% (11/20 getafe)\n",
      "-> 30.0% (12/20 osasuna)\n",
      "-> 32.5% (13/20 sevilla)\n",
      "-> 35.0% (14/20 villarreal)\n",
      "-> 37.5% (15/20 alaves)\n",
      "-> 40.0% (16/20 cadiz)\n",
      "-> 42.5% (17/20 mallorca)\n",
      "-> 45.0% (18/20 celta_vigo)\n",
      "-> 47.5% (19/20 granada)\n",
      "-> 50.0% (20/20 almeria)\n",
      "Scraping the season:  2022\n",
      "-> 52.5% (1/20 barcelona)\n",
      "-> 55.0% (2/20 real_madrid)\n",
      "-> 57.5% (3/20 atletico_madrid)\n",
      "-> 60.0% (4/20 real_sociedad)\n",
      "-> 62.5% (5/20 villarreal)\n",
      "-> 65.0% (6/20 real_betis)\n",
      "-> 67.5% (7/20 osasuna)\n",
      "-> 70.0% (8/20 athletic_club)\n",
      "-> 72.5% (9/20 mallorca)\n",
      "-> 75.0% (10/20 girona)\n",
      "-> 77.5% (11/20 rayo_vallecano)\n",
      "-> 80.0% (12/20 sevilla)\n",
      "-> 82.5% (13/20 celta_vigo)\n",
      "-> 85.0% (14/20 cadiz)\n",
      "-> 87.5% (15/20 getafe)\n",
      "-> 90.0% (16/20 valencia)\n",
      "-> 92.5% (17/20 almeria)\n",
      "-> 95.0% (18/20 valladolid)\n",
      "-> 97.5% (19/20 espanyol)\n",
      "-> 100.0% (20/20 elche)\n",
      "Index(['Date', 'Time', 'Comp', 'Round', 'Day', 'Venue', 'Result', 'GF', 'GA',\n",
      "       'Opponent', 'xG', 'xGA', 'Poss', 'Attendance', 'Captain', 'Formation',\n",
      "       'Referee', 'Match Report', 'Notes', 'Sh', 'SoT', 'Saves', 'Cmp', 'Cmp',\n",
      "       'Cmp', 'Cmp', 'Att', 'Att', 'Att', 'Att', 'PrgP', 'KP', 'pass_3rd',\n",
      "       'Sw', 'Crs', 'SCA', 'GCA', 'Tkl', 'Tkl', 'TklW', 'Tkl_Def_3rd',\n",
      "       'Tkl_Att_3rd', 'Blocks', 'Int', 'Touches_Att_3rd', 'Fls', 'Off',\n",
      "       'Recov', 'Season', 'Team'],\n",
      "      dtype='object')\n",
      "Index(['Date', 'Time', 'Comp', 'Round', 'Day', 'Venue', 'Result', 'GF', 'GA',\n",
      "       'Opponent', 'xG', 'xGA', 'Poss', 'Attendance', 'Captain', 'Formation',\n",
      "       'Referee', 'Match Report', 'Notes', 'Sh', 'SoT', 'Saves', 'Cmp', 'Cmp',\n",
      "       'Cmp', 'Cmp', 'Att', 'Att', 'Att', 'Att', 'PrgP', 'KP', 'pass_3rd',\n",
      "       'Sw', 'Crs', 'SCA', 'GCA', 'Tkl', 'Tkl', 'TklW', 'Tkl_Def_3rd',\n",
      "       'Tkl_Att_3rd', 'Blocks', 'Int', 'Touches_Att_3rd', 'Fls', 'Off',\n",
      "       'Recov', 'Season', 'Team'],\n",
      "      dtype='object')\n",
      "Getting the seasons:  [2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015]\n",
      "Scraping the season:  2023\n",
      "-> 0.62% (1/18 paris_saint_germain)\n",
      "-> 1.23% (2/18 nice)\n",
      "-> 1.85% (3/18 monaco)\n",
      "-> 2.47% (4/18 lille)\n",
      "-> 3.09% (5/18 reims)\n",
      "-> 3.7% (6/18 lens)\n",
      "-> 4.32% (7/18 le_havre)\n",
      "-> 4.94% (8/18 brest)\n",
      "-> 5.56% (9/18 nantes)\n",
      "-> 6.17% (10/18 montpellier)\n",
      "-> 6.79% (11/18 marseille)\n",
      "-> 7.41% (12/18 metz)\n",
      "-> 8.02% (13/18 rennes)\n",
      "-> 8.64% (14/18 toulouse)\n",
      "-> 9.26% (15/18 strasbourg)\n",
      "-> 9.88% (16/18 lorient)\n",
      "-> 10.49% (17/18 clermont_foot)\n",
      "-> 11.11% (18/18 lyon)\n",
      "Scraping the season:  2022\n",
      "-> 10.56% (1/20 paris_saint_germain)\n",
      "-> 11.11% (2/20 lens)\n",
      "-> 11.67% (3/20 marseille)\n",
      "-> 12.22% (4/20 rennes)\n",
      "-> 12.78% (5/20 lille)\n",
      "-> 13.33% (6/20 monaco)\n",
      "-> 13.89% (7/20 lyon)\n",
      "-> 14.44% (8/20 clermont_foot)\n",
      "-> 15.0% (9/20 nice)\n",
      "-> 15.56% (10/20 lorient)\n",
      "-> 16.11% (11/20 reims)\n",
      "-> 16.67% (12/20 montpellier)\n",
      "-> 17.22% (13/20 toulouse)\n",
      "-> 17.78% (14/20 brest)\n",
      "-> 18.33% (15/20 strasbourg)\n",
      "-> 18.89% (16/20 nantes)\n",
      "-> 19.44% (17/20 auxerre)\n",
      "-> 20.0% (18/20 ajaccio)\n",
      "-> 20.56% (19/20 troyes)\n",
      "-> 21.11% (20/20 angers)\n",
      "Scraping the season:  2021\n",
      "-> 21.67% (1/20 paris_saint_germain)\n",
      "-> 22.22% (2/20 marseille)\n",
      "-> 22.78% (3/20 monaco)\n",
      "-> 23.33% (4/20 rennes)\n",
      "-> 23.89% (5/20 nice)\n",
      "-> 24.44% (6/20 strasbourg)\n",
      "-> 25.0% (7/20 lens)\n",
      "-> 25.56% (8/20 lyon)\n",
      "-> 26.11% (9/20 nantes)\n",
      "-> 26.67% (10/20 lille)\n",
      "-> 27.22% (11/20 brest)\n",
      "-> 27.78% (12/20 reims)\n",
      "-> 28.33% (13/20 montpellier)\n",
      "-> 28.89% (14/20 angers)\n",
      "-> 29.44% (15/20 troyes)\n",
      "-> 30.0% (16/20 lorient)\n",
      "-> 30.56% (17/20 clermont_foot)\n",
      "-> 31.11% (18/20 saint_etienne)\n",
      "-> 31.67% (19/20 metz)\n",
      "-> 32.22% (20/20 bordeaux)\n",
      "Scraping the season:  2020\n",
      "-> 32.78% (1/20 lille)\n",
      "-> 33.33% (2/20 paris_saint_germain)\n",
      "-> 33.89% (3/20 monaco)\n",
      "-> 34.44% (4/20 lyon)\n",
      "-> 35.0% (5/20 marseille)\n",
      "-> 35.56% (6/20 rennes)\n",
      "-> 36.11% (7/20 lens)\n",
      "-> 36.67% (8/20 montpellier)\n",
      "-> 37.22% (9/20 nice)\n",
      "-> 37.78% (10/20 metz)\n",
      "-> 38.33% (11/20 saint_etienne)\n",
      "-> 38.89% (12/20 bordeaux)\n",
      "-> 39.44% (13/20 angers)\n",
      "-> 40.0% (14/20 reims)\n",
      "-> 40.56% (15/20 strasbourg)\n",
      "-> 41.11% (16/20 lorient)\n",
      "-> 41.67% (17/20 brest)\n",
      "-> 42.22% (18/20 nantes)\n",
      "-> 42.78% (19/20 nimes)\n",
      "-> 43.33% (20/20 dijon)\n",
      "Scraping the season:  2019\n",
      "-> 43.89% (1/20 paris_saint_germain)\n",
      "-> 44.44% (2/20 marseille)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\Meu Drive\\Bet\\scraper\\scraping.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m historico \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m historico\u001b[39m.\u001b[39mappend(match_history(ligas[\u001b[39m'\u001b[39m\u001b[39mla_liga\u001b[39m\u001b[39m'\u001b[39m], years\u001b[39m=\u001b[39m [\u001b[39m2023\u001b[39m, \u001b[39m2022\u001b[39m]))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m historico\u001b[39m.\u001b[39mappend(match_history(ligas[\u001b[39m'\u001b[39;49m\u001b[39mligue_1\u001b[39;49m\u001b[39m'\u001b[39;49m], years\u001b[39m=\u001b[39;49m anos))\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m historico \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(historico)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m historico_incomp \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_excel(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../datasets/xlsx/raw/historico.xlsx\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mg:\\Meu Drive\\Bet\\scraper\\scraping.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m links \u001b[39m=\u001b[39m [l\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m links \u001b[39m=\u001b[39m [l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m links \u001b[39mif\u001b[39;00m l \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mall_comps/keeper\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m l]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X13sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m data_goalkeeping \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttps://fbref.com\u001b[39;49m\u001b[39m{\u001b[39;49;00mlinks[\u001b[39m0\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X13sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m goalkeeping \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_html(data_goalkeeping\u001b[39m.\u001b[39mtext, match\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGoalkeeping\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/scraper/scraping.ipynb#X13sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m goalkeeping\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m goalkeeping\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mdroplevel()\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    468\u001b[0m     \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1096\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[1;32m-> 1096\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[0;32m   1099\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1100\u001b[0m         (\n\u001b[0;32m   1101\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mconn\u001b[39m.\u001b[39mhost\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1106\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1107\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\urllib3\\connection.py:642\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[39mif\u001b[39;00m is_time_off:\n\u001b[0;32m    634\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    635\u001b[0m         (\n\u001b[0;32m    636\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSystem time is way off (before \u001b[39m\u001b[39m{\u001b[39;00mRECENT_DATE\u001b[39m}\u001b[39;00m\u001b[39m). This will probably \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    639\u001b[0m         SystemTimeWarning,\n\u001b[0;32m    640\u001b[0m     )\n\u001b[1;32m--> 642\u001b[0m sock_and_verified \u001b[39m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[0;32m    643\u001b[0m     sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    644\u001b[0m     cert_reqs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_reqs,\n\u001b[0;32m    645\u001b[0m     ssl_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_version,\n\u001b[0;32m    646\u001b[0m     ssl_minimum_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_minimum_version,\n\u001b[0;32m    647\u001b[0m     ssl_maximum_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_maximum_version,\n\u001b[0;32m    648\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[0;32m    649\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[0;32m    650\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[0;32m    651\u001b[0m     cert_file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[0;32m    652\u001b[0m     key_file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[0;32m    653\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[0;32m    654\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    655\u001b[0m     ssl_context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_context,\n\u001b[0;32m    656\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[0;32m    657\u001b[0m     assert_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_hostname,\n\u001b[0;32m    658\u001b[0m     assert_fingerprint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_fingerprint,\n\u001b[0;32m    659\u001b[0m )\n\u001b[0;32m    660\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock_and_verified\u001b[39m.\u001b[39msocket\n\u001b[0;32m    661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_verified \u001b[39m=\u001b[39m sock_and_verified\u001b[39m.\u001b[39mis_verified\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\urllib3\\connection.py:782\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[39mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[0;32m    780\u001b[0m         server_hostname \u001b[39m=\u001b[39m normalized\n\u001b[1;32m--> 782\u001b[0m ssl_sock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    783\u001b[0m     sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    784\u001b[0m     keyfile\u001b[39m=\u001b[39;49mkey_file,\n\u001b[0;32m    785\u001b[0m     certfile\u001b[39m=\u001b[39;49mcert_file,\n\u001b[0;32m    786\u001b[0m     key_password\u001b[39m=\u001b[39;49mkey_password,\n\u001b[0;32m    787\u001b[0m     ca_certs\u001b[39m=\u001b[39;49mca_certs,\n\u001b[0;32m    788\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49mca_cert_dir,\n\u001b[0;32m    789\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49mca_cert_data,\n\u001b[0;32m    790\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    791\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[0;32m    792\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[0;32m    793\u001b[0m )\n\u001b[0;32m    795\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     \u001b[39mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\urllib3\\util\\ssl_.py:445\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m ca_certs \u001b[39mor\u001b[39;00m ca_cert_dir \u001b[39mor\u001b[39;00m ca_cert_data:\n\u001b[0;32m    444\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 445\u001b[0m         context\u001b[39m.\u001b[39;49mload_verify_locations(ca_certs, ca_cert_dir, ca_cert_data)\n\u001b[0;32m    446\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    447\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "historico = []\n",
    "historico.append(match_history(ligas['la_liga'], years= [2023, 2022]))\n",
    "historico.append(match_history(ligas['ligue_1'], years= anos))\n",
    "\n",
    "historico = pd.concat(historico)\n",
    "\n",
    "historico_incomp = pd.read_excel(f'../datasets/xlsx/raw/historico.xlsx')\n",
    "historico1 = pd.concat([historico_incomp, historico])\n",
    "historico1.to_excel('../datasets/xlsx/raw/historico.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  historico.append(match_history(ligas[liga], years= anos))\n",
    "\n",
    "historico = pd.concat(historico)\n",
    "\n",
    "historico.to_excel(f'../datasets/xlsx/raw/historico.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elenco = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  elenco.append(get_squads(ligas[liga], years= anos))\n",
    "\n",
    "elenco = pd.concat(elenco)\n",
    "\n",
    "elenco.to_excel(f'../datasets/xlsx/raw/elenco.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  tabela.append(standings(ligas[liga], years= anos))\n",
    "\n",
    "tabela = pd.concat(tabela)\n",
    "\n",
    "tabela.to_excel(f'../datasets/xlsx/raw/tabela.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodadas = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  rodadas.append(next_matches(ligas[liga]))\n",
    "\n",
    "rodadas = pd.concat(rodadas)\n",
    "\n",
    "rodadas.to_excel(f'../datasets/xlsx/raw/rodadas.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atualizar temporada atual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico_old = pd.read_excel(f'../datasets/xlsx/raw/historico.xlsx')\n",
    "\n",
    "historico = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  historico.append(match_history(ligas[liga], update= historico_old))\n",
    "\n",
    "historico = pd.concat(historico)\n",
    "\n",
    "historico.to_excel(f'datasets/xlsx/raw/historico.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_old = pd.read_excel(f'../datasets/xlsx/raw/tabela.xlsx')\n",
    "\n",
    "tabela = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  tabela.append(match_history(ligas[liga], update= tabela_old))\n",
    "\n",
    "tabela = pd.concat(tabela)\n",
    "\n",
    "tabela.to_excel(f'datasets/xlsx/raw/tabela.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elenco_old = pd.read_excel(f'../datasets/xlsx/raw/elenco.xlsx')\n",
    "\n",
    "elenco = []\n",
    "for liga in ligas:\n",
    "  print(f'Scraping the league: {ligas[liga][2]}')\n",
    "  elenco.append(get_squads(ligas[liga], update= elenco_old))\n",
    "\n",
    "elenco = pd.concat(elenco)\n",
    "\n",
    "elenco.to_excel(f'datasets/xlsx/raw/elenco.xlsx', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
