{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "#from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HISTORICO DE PARTIDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historico_partidas(tabela_url, paginas):\n",
    "  todas_partidas = []\n",
    "  data = requests.get(tabela_url) # Pega o html (em forma de string) da pagina 'Tabela do Brasileirao'\n",
    "  soup = BeautifulSoup(data.text) # Converte o String em html\n",
    "  tabela_brasileirao = soup.select('table.stats_table')[0] # Seleciona dentro do HTML a tabela em si\n",
    "  links_times = tabela_brasileirao.find_all('a') # Procura a Anchor que contem todos os links do time\n",
    "  links_times = [link.get(\"href\") for link in links_times] # Pega só os links em si\n",
    "  links_times = [link for link in links_times if paginas[0] in link] # Escolhe o link que leva para a pagina de Stats do time\n",
    "  times_urls = [f\"https://fbref.com{link}\" for link in links_times] # Transforma a string em URL\n",
    "\n",
    "  # Para cada time faça:\n",
    "  ## 1- Pega o nome do time\n",
    "  ## 2- Pega as informações das abas (em ordem):\n",
    "  ### Scores & Fixtures, Shooting, Goalkeeping, Passing, Pass Types, Goal and Shot Creation, Defensive Actions, Possession, Miscellaneous Stats\n",
    "  ## 3- Transforma a tabela que esta em HTML em Dataframe\n",
    "  for time_url in times_urls:\n",
    "    nome_time = time_url.split('/')[-1].replace(\"-Stats\", \"\")\n",
    "    data = requests.get(time_url) # Pega o html (em forma de string) da pagina do time\n",
    "    soup = BeautifulSoup(data.text)\n",
    "\n",
    "    ## Scores & Fixtures\n",
    "    partidas = pd.read_html(data.text, match= 'Scores & Fixtures')[0] # Transforma em dataframe o historico de partidas\n",
    "    \n",
    "    ## Shooting\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[1] in l]\n",
    "    data_shooting = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    shooting = pd.read_html(data_shooting.text, match= \"Shooting\")[0]\n",
    "    shooting.columns = shooting.columns.droplevel()\n",
    "    \n",
    "    ## Goalkeeping\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[2] in l]\n",
    "    data_goalkeeping = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    goalkeeping = pd.read_html(data_goalkeeping.text, match= \"Goalkeeping\")[0]\n",
    "    goalkeeping.columns = goalkeeping.columns.droplevel()\n",
    "    \n",
    "    ## Passing\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[3] in l]\n",
    "    data_passing = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    passing = pd.read_html(data_passing.text, match= \"Passing\")[0]\n",
    "    passing.columns = passing.columns.droplevel()\n",
    "    \n",
    "    ## Pass Types\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[4] in l]\n",
    "    data_passtype = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    pass_types = pd.read_html(data_passtype.text, match= \"Pass Types\")[0]\n",
    "    pass_types.columns = pass_types.columns.droplevel()\n",
    "    \n",
    "    ## Goal and Shot Creation\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[5] in l]\n",
    "    data_shotcreation = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    goal_shotcreation = pd.read_html(data_shotcreation.text, match= \"Goal and Shot Creation\")[0]\n",
    "    goal_shotcreation.columns = goal_shotcreation.columns.droplevel()\n",
    "    \n",
    "    ## Defensive Actions\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and paginas[6] in l]\n",
    "    data_defensive = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    defensive = pd.read_html(data_defensive.text, match= \"Defensive Actions\")[0]\n",
    "    defensive.columns = defensive.columns.droplevel()\n",
    "    \n",
    "    ## Possession\n",
    "    ## Miscellaneous Stats\n",
    "\n",
    "    # Merge das abas\n",
    "    try:\n",
    "      time_df = partidas.merge(\n",
    "        shooting[['Date', 'Sh', 'SoT', 'SoT%', 'G/Sh', 'G/SoT', 'FK', 'PK', 'PKatt', 'npxG', 'npxG/Sh', 'G-xG', 'np:G-xG']], on= 'Date')\n",
    "      time_df = time_df.merge(\n",
    "        goalkeeping[['Date', 'SoTA', 'Saves', 'Save%', 'CS', 'PSxG', 'PSxG+/-']], on= 'Date')\n",
    "      time_df = time_df.merge(\n",
    "        passing[['Date', 'TotDist', 'PrgDist', 'Ast',\t'xAG', 'xA', 'KP', '1/3', 'PPA', 'CrsPA', 'PrgP']], on= 'Date') \n",
    "      time_df = time_df.merge(\n",
    "        pass_types[['Date', 'TB', 'Sw', 'Crs', 'Blocks']], on= 'Date')\n",
    "      time_df = time_df.merge(\n",
    "        goal_shotcreation[['Date', 'SCA', 'GCA']], on= 'Date')\n",
    "      time_df = time_df.merge(\n",
    "        defensive[['Date', 'TklW','Def 3rd','Mid 3rd','Att 3rd','Int','Err']], on= 'Date')\n",
    "      #time_df = time_df.merge(\n",
    "        #possession[['Date', ]], on= 'Date')\n",
    "      #time_df = time_df.merge(\n",
    "        #miscellaneous[['Date',]], on= 'Date')\n",
    "    except ValueError:\n",
    "      continue\n",
    "    \n",
    "    time_df = time_df[time_df[\"Comp\"] == \"Série A\"] # Filtra o brasileirão\n",
    "    time_df['Equipe'] = nome_time\n",
    "    todas_partidas.append(time_df)\n",
    "    time.sleep(10)\n",
    "\n",
    "  historico = pd.concat(todas_partidas)\n",
    "  return historico\n",
    "\n",
    "tabela = \"https://fbref.com/en/comps/24/Serie-A-Stats\"\n",
    "abas = ['/squads/', 'all_comps/shooting/', 'all_comps/keeper', 'all_comps/passing', 'all_comps/passing_types', 'all_comps/gca', 'all_comps/defense', 'all_comps/possession', 'all_comps/misc']\n",
    "historico = historico_partidas(tabela, abas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico.to_excel(f'dados/historico/historico-naotratado.xlsx', sheet_name='historico', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABELA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabela_brasileirao(tabela_url):\n",
    "  data = requests.get(tabela_url)\n",
    "  tabela = pd.read_html(data.text, match= 'Regular season')[0]\n",
    "  return tabela\n",
    "\n",
    "tabela_url = \"https://fbref.com/en/comps/24/Serie-A-Stats\"\n",
    "tabela_df = tabela_brasileirao(tabela_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodada = tabela_df['MP'].max()\n",
    "tabela_df.to_excel(f'dados/tabela/tabela-naotratado.xlsx', sheet_name='tabela', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximas Rodadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxs_rodadas(tabela_url):\n",
    "  data = requests.get(tabela_url)\n",
    "  tabela = pd.read_html(data.text, match= 'Scores & Fixtures')[0]\n",
    "  return tabela\n",
    "\n",
    "rodadas_url = \"https://fbref.com/en/comps/24/schedule/Serie-A-Scores-and-Fixtures\"\n",
    "rodadas_df = proxs_rodadas(rodadas_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodadas_df.to_excel(f'dados/rodadas/todas_rodadas-naotratado.xlsx', sheet_name='tabela', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
