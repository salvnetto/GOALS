{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HISTORICO DE PARTIDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opções disponiveis:\n",
    " - Brasileirão Serie A  [2023 a 2014]\n",
    " - Premier League       [2023 a 2014]  *2014 significa a temporada 2015-2014*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_history(league, years = [2023], update = None):\n",
    "  print(\"Getting the seasons: \", years)\n",
    "  match_history = []\n",
    "  for year in years:\n",
    "    print(\"Scraping the season: \", year)\n",
    "    data = requests.get(league[1])\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    tabela = soup.select('table.stats_table')[0]                        #Seleciona a tabela principal\n",
    "    links = tabela.find_all('a')                            #Procura a Anchor que contem todos os links do time\n",
    "    links = [link.get('href') for link in links]            #Pega os links (sem o começo deles)\n",
    "    links = [link for link in links if '/squads/' in link]  #Pega apenas o link 'squads'\n",
    "    urls = [f\"https://fbref.com{link}\" for link in links]   #Adiciona o inicio do html\n",
    "\n",
    "    try:\n",
    "      prev_season = soup.select('a.prev')[0].get('href')      #Vai para a temporada anterior\n",
    "      league[1] = f\"https://fbref.com{prev_season}\"\n",
    "    except IndexError:\n",
    "      continue\n",
    "\n",
    "    #Itera sobre todos os times da tabela\n",
    "    for team in urls:\n",
    "      nome_time = team.split('/')[-1].replace('-Stats', '').replace('-','_').lower()\n",
    "      data = requests.get(team)\n",
    "      soup = BeautifulSoup(data.text)\n",
    "\n",
    "      ##Partidas jogadas\n",
    "      matches = pd.read_html(data.text, match= 'Scores & Fixtures')[0]\n",
    "\n",
    "      ##Chutes\n",
    "      links = [link.get(\"href\") for link in soup.find_all('a')]\n",
    "      links = [link for link in links if link and 'all_comps/shooting/' in link]\n",
    "      data_shooting = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      shooting = pd.read_html(data_shooting.text, match= \"Shooting\")[0]\n",
    "      shooting.columns = shooting.columns.droplevel()\n",
    "\n",
    "      ##Goleiros\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/keeper' in l]\n",
    "      #data_goalkeeping = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #goalkeeping = pd.read_html(data_goalkeeping.text, match= \"Goalkeeping\")[0]\n",
    "      #goalkeeping.columns = goalkeeping.columns.droplevel()\n",
    "\n",
    "      ##Passes\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/passing' in l]\n",
    "      #data_passing = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #passing = pd.read_html(data_passing.text, match= \"Passing\")[0]\n",
    "      #passing.columns = passing.columns.droplevel()\n",
    "\n",
    "      ##Tipos de Passes\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/passing_types' in l]\n",
    "      #data_passtype = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #pass_types = pd.read_html(data_passtype.text, match= \"Pass Types\")[0]\n",
    "      #pass_types.columns = pass_types.columns.droplevel()\n",
    "\n",
    "      ##Gols e Criação de Chutes\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/gca' in l]\n",
    "      #data_shotcreation = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #goal_shotcreation = pd.read_html(data_shotcreation.text, match= \"Goal and Shot Creation\")[0]\n",
    "      #goal_shotcreation.columns = goal_shotcreation.columns.droplevel()\n",
    "\n",
    "      ##Acoes Defensivas\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/defense' in l]\n",
    "      #data_defensive = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #defensive = pd.read_html(data_defensive.text, match= \"Defensive Actions\")[0]\n",
    "      #defensive.columns = defensive.columns.droplevel()\n",
    "\n",
    "      ##Posse de Bola\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/possession' in l]\n",
    "      #data_possession = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #possession = pd.read_html(data_possession.text, match= \"Possession\")[0]\n",
    "      #possession.columns = possession.columns.droplevel()\n",
    "\n",
    "      ##Demais Estatisticas\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/misc' in l]\n",
    "      #data_misc = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #misc = pd.read_html(data_misc.text, match= \"Miscellaneous Stats\")[0]\n",
    "      #misc.columns = misc.columns.droplevel()\n",
    "\n",
    "\n",
    "      # Merge das abas\n",
    "      try:\n",
    "        matches_df = matches.merge(\n",
    "          shooting[['Date', 'Sh', 'SoT']], on= 'Date')\n",
    "        #matches_df = matches_df.merge(\n",
    "          #goalkeeping[['Date', ]], on= 'Date')\n",
    "        #matches_df = matches_df.merge(\n",
    "          #passing[['Date', ]], on= 'Date') \n",
    "        #matches_df = matches_df.merge(\n",
    "          #pass_types[['Date', ]], on= 'Date')\n",
    "        #matches_df = matches_df.merge(\n",
    "          #goal_shotcreation[['Date', ]], on= 'Date')\n",
    "        #matches_df = matches_df.merge(\n",
    "          #defensive[['Date', ]], on= 'Date')\n",
    "        #time_df = time_df.merge(\n",
    "          #possession[['Date', ]], on= 'Date')\n",
    "        #time_df = time_df.merge(\n",
    "          #miscellaneous[['Date',]], on= 'Date')\n",
    "      except ValueError:\n",
    "        continue\n",
    "\n",
    "      matches_df = matches_df[matches_df[\"Comp\"] == league[0]]\n",
    "      matches_df['Season'] = year\n",
    "      matches_df['Team'] = nome_time\n",
    "      \n",
    "      if update is None:\n",
    "        match_history.append(matches_df)\n",
    "      else:\n",
    "        update.drop(update[update['season'] == year].index, inplace= True)\n",
    "        update.append(matches_df)\n",
    "        match_history = update\n",
    "\n",
    "      time.sleep(10)\n",
    "    \n",
    "  if update is None:\n",
    "    match_history = pd.concat(match_history)\n",
    "  match_history.columns = [c.lower() for c in match_history.columns]\n",
    "\n",
    "  return match_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligas = {'brasileirao_a': [\"Série A\", \"https://fbref.com/en/comps/24/Serie-A-Stats\"],\n",
    "         'premier_league': [\"Premier League\", \"https://fbref.com/en/comps/9/Premier-League-Stats\"],}\n",
    "\n",
    "anos = list(range(2023, 2013, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brasileirao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the seasons:  [2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015, 2014]\n",
      "Scraping the season:  2023\n",
      "Scraping the season:  2022\n",
      "Scraping the season:  2021\n",
      "Scraping the season:  2020\n",
      "Scraping the season:  2019\n",
      "Scraping the season:  2018\n",
      "Scraping the season:  2017\n",
      "Scraping the season:  2016\n",
      "Scraping the season:  2015\n",
      "Scraping the season:  2014\n"
     ]
    }
   ],
   "source": [
    "# Baixando o Brasileirao de 2014 a 2023\n",
    "##historico_brasileirao = match_history(ligas['brasileirao_a'], years= anos)\n",
    "\n",
    "# Atualizando o Brasileirao de 2023\n",
    "##historico_brasileirao = match_history(ligas['brasileirao_a'], update= historico_brasileirao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico_brasileirao.to_excel('datasets/Brasileirao/match_history/historico-2015a2023(naotratada).xlsx', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premier League"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the seasons:  [2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015, 2014]\n",
      "Scraping the season:  2023\n",
      "Scraping the season:  2022\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No tables found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mg:\\Meu Drive\\Bet\\1-scrap.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/1-scrap.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Baixando a Premier League de 2014 a 2023\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/1-scrap.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m historico_premierleague \u001b[39m=\u001b[39m match_history(ligas[\u001b[39m'\u001b[39;49m\u001b[39mpremier_league\u001b[39;49m\u001b[39m'\u001b[39;49m], anos)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/1-scrap.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Atualizando a Premier League de 2023\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/1-scrap.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m##historico_premierleague = match_history(ligas['premier_league'])\u001b[39;00m\n",
      "\u001b[1;32mg:\\Meu Drive\\Bet\\1-scrap.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/1-scrap.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(data\u001b[39m.\u001b[39mtext)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/1-scrap.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m##Partidas jogadas\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/1-scrap.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m matches \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_html(data\u001b[39m.\u001b[39;49mtext, match\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mScores & Fixtures\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/1-scrap.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m##Chutes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Meu%20Drive/Bet/1-scrap.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m links \u001b[39m=\u001b[39m [link\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)]\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\pandas\\io\\html.py:1205\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links)\u001b[0m\n\u001b[0;32m   1201\u001b[0m validate_header_arg(header)\n\u001b[0;32m   1203\u001b[0m io \u001b[39m=\u001b[39m stringify_path(io)\n\u001b[1;32m-> 1205\u001b[0m \u001b[39mreturn\u001b[39;00m _parse(\n\u001b[0;32m   1206\u001b[0m     flavor\u001b[39m=\u001b[39;49mflavor,\n\u001b[0;32m   1207\u001b[0m     io\u001b[39m=\u001b[39;49mio,\n\u001b[0;32m   1208\u001b[0m     match\u001b[39m=\u001b[39;49mmatch,\n\u001b[0;32m   1209\u001b[0m     header\u001b[39m=\u001b[39;49mheader,\n\u001b[0;32m   1210\u001b[0m     index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m   1211\u001b[0m     skiprows\u001b[39m=\u001b[39;49mskiprows,\n\u001b[0;32m   1212\u001b[0m     parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m   1213\u001b[0m     thousands\u001b[39m=\u001b[39;49mthousands,\n\u001b[0;32m   1214\u001b[0m     attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1215\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1216\u001b[0m     decimal\u001b[39m=\u001b[39;49mdecimal,\n\u001b[0;32m   1217\u001b[0m     converters\u001b[39m=\u001b[39;49mconverters,\n\u001b[0;32m   1218\u001b[0m     na_values\u001b[39m=\u001b[39;49mna_values,\n\u001b[0;32m   1219\u001b[0m     keep_default_na\u001b[39m=\u001b[39;49mkeep_default_na,\n\u001b[0;32m   1220\u001b[0m     displayed_only\u001b[39m=\u001b[39;49mdisplayed_only,\n\u001b[0;32m   1221\u001b[0m     extract_links\u001b[39m=\u001b[39;49mextract_links,\n\u001b[0;32m   1222\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\pandas\\io\\html.py:1006\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1005\u001b[0m     \u001b[39massert\u001b[39;00m retained \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# for mypy\u001b[39;00m\n\u001b[1;32m-> 1006\u001b[0m     \u001b[39mraise\u001b[39;00m retained\n\u001b[0;32m   1008\u001b[0m ret \u001b[39m=\u001b[39m []\n\u001b[0;32m   1009\u001b[0m \u001b[39mfor\u001b[39;00m table \u001b[39min\u001b[39;00m tables:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\pandas\\io\\html.py:986\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m p \u001b[39m=\u001b[39m parser(io, compiled_match, attrs, encoding, displayed_only, extract_links)\n\u001b[0;32m    985\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 986\u001b[0m     tables \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mparse_tables()\n\u001b[0;32m    987\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m caught:\n\u001b[0;32m    988\u001b[0m     \u001b[39m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[0;32m    989\u001b[0m     \u001b[39m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[0;32m    990\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(io, \u001b[39m\"\u001b[39m\u001b[39mseekable\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m io\u001b[39m.\u001b[39mseekable():\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\pandas\\io\\html.py:262\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_tables\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    255\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m     tables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_tables(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_doc(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattrs)\n\u001b[0;32m    263\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[39mfor\u001b[39;00m table \u001b[39min\u001b[39;00m tables)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\pandas\\io\\html.py:618\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._parse_tables\u001b[1;34m(self, doc, match, attrs)\u001b[0m\n\u001b[0;32m    615\u001b[0m tables \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mfind_all(element_name, attrs\u001b[39m=\u001b[39mattrs)\n\u001b[0;32m    617\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tables:\n\u001b[1;32m--> 618\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo tables found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    620\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m    621\u001b[0m unique_tables \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: No tables found"
     ]
    }
   ],
   "source": [
    "# Baixando a Premier League de 2014 a 2023\n",
    "historico_premierleague = match_history(ligas['premier_league'], anos)\n",
    "\n",
    "# Atualizando a Premier League de 2023\n",
    "##historico_premierleague = match_history(ligas['premier_league'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico_premierleague.to_excel('datasets/Premier_League/match_history/historico-2015a2023(naotratada).xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABELA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabela_brasileirao(tabela_url):\n",
    "  data = requests.get(tabela_url)\n",
    "  tabela = pd.read_html(data.text, match= 'Regular season')[0]\n",
    "  return tabela\n",
    "\n",
    "tabela_url = \"https://fbref.com/en/comps/24/Serie-A-Stats\"\n",
    "tabela_df = tabela_brasileirao(tabela_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodada = tabela_df['MP'].max()\n",
    "tabela_df.to_excel(f'dados/tabela/tabela-naotratado.xlsx', sheet_name='tabela', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximas Rodadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxs_rodadas(tabela_url):\n",
    "  data = requests.get(tabela_url)\n",
    "  tabela = pd.read_html(data.text, match= 'Scores & Fixtures')[0]\n",
    "  return tabela\n",
    "\n",
    "rodadas_url = \"https://fbref.com/en/comps/24/schedule/Serie-A-Scores-and-Fixtures\"\n",
    "rodadas_df = proxs_rodadas(rodadas_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodadas_df.to_excel(f'dados/rodadas/todas_rodadas-naotratado.xlsx', sheet_name='tabela', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
