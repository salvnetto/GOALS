{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opções disponiveis:\n",
    " - Brasileirão Serie A  [2023 a 2015]\n",
    " - Premier League       [2023 a 2015]  *2015 significa a temporada 2016-2015*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligas = {'brasileirao_a': [\"Série A\", \"https://fbref.com/en/comps/24/Serie-A-Stats\", 'Brasileirao', 'https://fbref.com/en/comps/24/schedule/Serie-A-Scores-and-Fixtures'],\n",
    "         'premier_league': [\"Premier League\", \"https://fbref.com/en/comps/9/Premier-League-Stats\", 'Premier_League', 'https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures'],}\n",
    "\n",
    "anos = list(range(2023, 2014, -1))\n",
    "\n",
    "season_atual = 2023\n",
    "liga_atual = ligas['premier_league']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liga Selecionada: \\\n",
    "Anos Selecionados: [2023 a 2014]\\\n",
    "Season Atual: 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_history(league, years = [2023], update = None):\n",
    "  print(\"Getting the seasons: \", years)\n",
    "  liga = league[1]\n",
    "  match_history = []\n",
    "  for year in years:\n",
    "    print(\"Scraping the season: \", year)\n",
    "    data = requests.get(liga)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    tabela = soup.select('table.stats_table')[0]                        #Seleciona a tabela principal\n",
    "    links = tabela.find_all('a')                            #Procura a Anchor que contem todos os links do time\n",
    "    links = [link.get('href') for link in links]            #Pega os links (sem o começo deles)\n",
    "    links = [link for link in links if '/squads/' in link]  #Pega apenas o link 'squads'\n",
    "    urls = [f\"https://fbref.com{link}\" for link in links]   #Adiciona o inicio do html\n",
    "\n",
    "    try:\n",
    "      prev_season = soup.select('a.prev')[0].get('href')      #Vai para a temporada anterior\n",
    "      liga = f\"https://fbref.com{prev_season}\"\n",
    "    except IndexError:\n",
    "      continue\n",
    "\n",
    "    #Itera sobre todos os times da tabela\n",
    "    for team in urls:\n",
    "      nome_time = team.split('/')[-1].replace('-Stats', '').replace('-','_').lower()\n",
    "      data = requests.get(team)\n",
    "      soup = BeautifulSoup(data.text)\n",
    "\n",
    "      ##Partidas jogadas\n",
    "      matches = pd.read_html(data.text, match= 'Scores & Fixtures')[0]\n",
    "\n",
    "      ##Chutes\n",
    "      links = [link.get(\"href\") for link in soup.find_all('a')]\n",
    "      links = [link for link in links if link and 'all_comps/shooting/' in link]\n",
    "      data_shooting = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      shooting = pd.read_html(data_shooting.text, match= \"Shooting\")[0]\n",
    "      shooting.columns = shooting.columns.droplevel()\n",
    "\n",
    "      ##Goleiros\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/keeper' in l]\n",
    "      #data_goalkeeping = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #goalkeeping = pd.read_html(data_goalkeeping.text, match= \"Goalkeeping\")[0]\n",
    "      #goalkeeping.columns = goalkeeping.columns.droplevel()\n",
    "\n",
    "      ##Passes\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/passing' in l]\n",
    "      #data_passing = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #passing = pd.read_html(data_passing.text, match= \"Passing\")[0]\n",
    "      #passing.columns = passing.columns.droplevel()\n",
    "\n",
    "      ##Tipos de Passes\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/passing_types' in l]\n",
    "      #data_passtype = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #pass_types = pd.read_html(data_passtype.text, match= \"Pass Types\")[0]\n",
    "      #pass_types.columns = pass_types.columns.droplevel()\n",
    "\n",
    "      ##Gols e Criação de Chutes\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/gca' in l]\n",
    "      #data_shotcreation = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #goal_shotcreation = pd.read_html(data_shotcreation.text, match= \"Goal and Shot Creation\")[0]\n",
    "      #goal_shotcreation.columns = goal_shotcreation.columns.droplevel()\n",
    "\n",
    "      ##Acoes Defensivas\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/defense' in l]\n",
    "      #data_defensive = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #defensive = pd.read_html(data_defensive.text, match= \"Defensive Actions\")[0]\n",
    "      #defensive.columns = defensive.columns.droplevel()\n",
    "\n",
    "      ##Posse de Bola\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/possession' in l]\n",
    "      #data_possession = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #possession = pd.read_html(data_possession.text, match= \"Possession\")[0]\n",
    "      #possession.columns = possession.columns.droplevel()\n",
    "\n",
    "      ##Demais Estatisticas\n",
    "      #links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "      #links = [l for l in links if l and 'all_comps/misc' in l]\n",
    "      #data_misc = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "      #misc = pd.read_html(data_misc.text, match= \"Miscellaneous Stats\")[0]\n",
    "      #misc.columns = misc.columns.droplevel()\n",
    "\n",
    "\n",
    "      # Merge das abas\n",
    "      try:\n",
    "        matches_df = matches.merge(\n",
    "          shooting[['Date', 'Sh', 'SoT']], on= 'Date')\n",
    "        #matches_df = matches_df.merge(\n",
    "          #goalkeeping[['Date', ]], on= 'Date')\n",
    "        #matches_df = matches_df.merge(\n",
    "          #passing[['Date', ]], on= 'Date') \n",
    "        #matches_df = matches_df.merge(\n",
    "          #pass_types[['Date', ]], on= 'Date')\n",
    "        #matches_df = matches_df.merge(\n",
    "          #goal_shotcreation[['Date', ]], on= 'Date')\n",
    "        #matches_df = matches_df.merge(\n",
    "          #defensive[['Date', ]], on= 'Date')\n",
    "        #time_df = time_df.merge(\n",
    "          #possession[['Date', ]], on= 'Date')\n",
    "        #time_df = time_df.merge(\n",
    "          #miscellaneous[['Date',]], on= 'Date')\n",
    "      except ValueError:\n",
    "        continue\n",
    "\n",
    "      matches_df = matches_df[matches_df[\"Comp\"] == league[0]]\n",
    "      matches_df['Season'] = year\n",
    "      matches_df['Team'] = nome_time\n",
    "      \n",
    "      match_history.append(matches_df)\n",
    "      time.sleep(10)\n",
    "    \n",
    "  match_history = pd.concat(match_history)\n",
    "  match_history.columns = [c.lower() for c in match_history.columns]\n",
    "  if update is not None:\n",
    "    update.drop(update[update['season'] == year].index, inplace= True)\n",
    "    update.columns = [c.lower() for c in update.columns]\n",
    "    match_history = pd.concat([match_history, update], axis = 0)\n",
    "\n",
    "  return match_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standings(league, years= [2023], update = None):\n",
    "  table = []\n",
    "  liga = league[1]\n",
    "  print(\"Getting the seasons: \", years)\n",
    "  for year in years:\n",
    "    print(\"Scraping the season: \", year)\n",
    "    data = requests.get(liga)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    table_df = pd.read_html(data.text, match= 'Regular season')[0]\n",
    "    table_df['Season'] = year\n",
    "  \n",
    "    table.append(table_df)\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "      prev_season = soup.select('a.prev')[0].get('href')      #Vai para a temporada anterior\n",
    "      liga = f\"https://fbref.com{prev_season}\"\n",
    "    except IndexError:\n",
    "      continue\n",
    "  \n",
    "  table = pd.concat(table)\n",
    "  table.columns = [c.lower() for c in table.columns]\n",
    "  if update is not None:\n",
    "    update.drop(update[update['season'] == year].index, inplace= True)\n",
    "    update.columns = [c.lower() for c in update.columns]\n",
    "    table = pd.concat([table, update], axis = 0)\n",
    "\n",
    "  return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_matches(league):\n",
    "  data = requests.get(league[3])\n",
    "  matches = pd.read_html(data.text, match= 'Scores & Fixtures')[0]\n",
    "  matches.columns = [c.lower() for c in matches.columns]\n",
    "  return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixar anos anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the seasons:  [2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015]\n",
      "Scraping the season:  2023\n",
      "Scraping the season:  2022\n",
      "Scraping the season:  2021\n",
      "Scraping the season:  2020\n",
      "Scraping the season:  2019\n",
      "Scraping the season:  2018\n",
      "Scraping the season:  2017\n",
      "Scraping the season:  2016\n",
      "Scraping the season:  2015\n"
     ]
    }
   ],
   "source": [
    "historico = match_history(liga_atual, years= anos)\n",
    "historico.to_excel(f'datasets/{liga_atual[2]}/match_history/historico-2015a2023(naotratada).xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the seasons:  [2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015]\n",
      "Scraping the season:  2023\n",
      "Scraping the season:  2022\n",
      "Scraping the season:  2021\n",
      "Scraping the season:  2020\n",
      "Scraping the season:  2019\n",
      "Scraping the season:  2018\n",
      "Scraping the season:  2017\n",
      "Scraping the season:  2016\n",
      "Scraping the season:  2015\n"
     ]
    }
   ],
   "source": [
    "tabela = standings(liga_atual, years= anos)\n",
    "tabela.to_excel(f'datasets/{liga_atual[2]}/standings/tabela-2015a2023(naotratada).xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodadas = next_matches(liga_atual)\n",
    "\n",
    "rodadas.to_excel(f'datasets/{liga_atual[2]}/rounds/prox_partidas(naotratada).xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atualizar temporada atual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the seasons:  [2023]\n",
      "Scraping the season:  2023\n"
     ]
    }
   ],
   "source": [
    "historico = pd.read_excel(f'datasets/{liga_atual[2]}/match_history/historico-2015a2023(naotratada).xlsx')\n",
    "\n",
    "historico = match_history(liga_atual, update= historico)\n",
    "\n",
    "historico.to_excel(f'datasets/{liga_atual[2]}/match_history/historico-2015a2023(naotratada).xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the seasons:  [2023]\n",
      "Scraping the season:  2023\n"
     ]
    }
   ],
   "source": [
    "tabela = pd.read_excel(f'datasets/{liga_atual[2]}/standings/tabela-2015a2023(naotratada).xlsx')\n",
    "\n",
    "tabela = standings(liga_atual, update= tabela)\n",
    "\n",
    "tabela.to_excel(f'datasets/{liga_atual[2]}/standings/tabela-2015a2023(naotratada).xlsx', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
